Directory structure:
└── parth-p-parekh-seedlings/
    ├── README.md
    ├── backend/
    │   ├── __init__.py
    │   ├── main.py
    │   ├── requirements.txt
    │   └── app/
    │       ├── __init__.py
    │       ├── main.py
    │       ├── api/
    │       │   ├── __init__.py
    │       │   └── routes.py
    │       ├── core/
    │       │   ├── __init__.py
    │       │   ├── config.py
    │       │   └── prompts.py
    │       ├── models/
    │       │   ├── __init__.py
    │       │   └── schemas.py
    │       ├── services/
    │       │   ├── __init__.py
    │       │   ├── github_service.py
    │       │   └── llm_service.py
    │       └── utils/
    │           ├── __init__.py
    │           ├── cache.py
    │           └── logger.py
    ├── frontend/
    │   ├── app.py
    │   ├── pages/
    │   │   ├── analytics.py
    │   │   └── home.py
    │   └── utils/
    │       ├── __init__.py
    │       ├── api_client.py
    │       └── formatters.py
    ├── scripts/
    │   ├── deploy.sh
    │   └── setup.sh
    └── tests/
        ├── __init__.py
        ├── test_api.py
        ├── test_github_service.py
        └── test_llm_service.py

================================================
FILE: README.md
================================================
# README - to be filled from README_md.txt



================================================
FILE: backend/__init__.py
================================================
"""
GitHub Issue Analyzer Backend
Main package initialization
"""

__version__ = "1.0.0"
__author__ = "GitHub Issue Analyzer Team"
__description__ = "AI-powered GitHub Issue Analysis using Gemini API"

# Package metadata
__all__ = ["app"]


================================================
FILE: backend/main.py
================================================
# Entry point - can be copy of app/main.py



================================================
FILE: backend/requirements.txt
================================================
# Python dependencies - to be filled from requirements_txt.txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
pydantic-settings==2.1.0

# Google Gemini API
google-generativeai==0.3.0

# GitHub API
PyGithub==2.1.1
httpx==0.25.2

# Caching and Data
redis==5.0.0
python-dotenv==1.0.0

# Async Support
asyncio==3.4.3

# Logging and Utilities
rich==13.7.0

# Development & Testing
pytest==7.4.3
pytest-asyncio==0.21.1
pytest-cov==4.1.0

# Type Checking
mypy==1.7.0

# Code Quality
black==23.12.0
ruff==0.1.8


================================================
FILE: backend/app/__init__.py
================================================
"""
GitHub Issue Analyzer - FastAPI Application
Core application package
"""

from fastapi import FastAPI
from typing import Optional

__version__ = "1.0.0"

# Lazy imports for performance
_app: Optional[FastAPI] = None

def get_app() -> FastAPI:
    """Get or create FastAPI application instance"""
    global _app
    if _app is None:
        from app.core.config import settings
        from app.api.routes import router
        from app.utils.logger import setup_logger
        
        setup_logger()
        _app = FastAPI(
            title="GitHub Issue Analyzer API",
            description="AI-powered analysis of GitHub issues using Google Gemini",
            version=__version__,
            docs_url="/docs",
            openapi_url="/openapi.json"
        )
        _app.include_router(router, prefix="/api/v1")
    return _app

__all__ = ["get_app"]



================================================
FILE: backend/app/main.py
================================================
# Main application entry point - to be filled from main_app_py.txt
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from contextlib import asynccontextmanager
import logging

from app.core.config import settings
from app.api.routes import router
from app.utils.logger import setup_logger

logger = setup_logger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifespan context manager for startup/shutdown."""
    # Startup
    logger.info("Starting GitHub Issue Analyzer Backend")
    settings.validate()
    logger.info(f"Configuration validated. API Version: {settings.API_VERSION}")
    yield
    # Shutdown
    logger.info("Shutting down GitHub Issue Analyzer Backend")


# Create FastAPI app with lifespan
app = FastAPI(
    title=settings.API_TITLE,
    description=settings.API_DESCRIPTION,
    version=settings.API_VERSION,
    lifespan=lifespan
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Global exception handler
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """Handle all unhandled exceptions."""
    logger.error(f"Unhandled exception: {exc}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={
            "success": False,
            "error": "Internal server error",
            "details": str(exc) if settings.DEBUG else None
        }
    )


# Include routers
app.include_router(router)


@app.get("/")
async def root():
    """Root endpoint with API info."""
    return {
        "name": settings.API_TITLE,
        "version": settings.API_VERSION,
        "docs": "/docs",
        "health": "/api/v1/health",
        "endpoints": {
            "analyze": "POST /api/v1/analyze",
            "analyze_batch": "POST /api/v1/analyze-batch",
            "stats": "GET /api/v1/stats",
            "health": "GET /api/v1/health"
        }
    }


if __name__ == "__main__":
    import uvicorn
    print("Hello")
    logger.info(f"Starting server on {settings.HOST}:{settings.PORT}")
    uvicorn.run(
        "app.main:app",
        host=settings.HOST,
        port=settings.PORT,
        reload=settings.RELOAD,
        log_level=settings.LOG_LEVEL.lower()
    )


================================================
FILE: backend/app/api/__init__.py
================================================
"""
API module - FastAPI routes and endpoints
"""

__all__ = ["router"]



================================================
FILE: backend/app/api/routes.py
================================================
# API routes - to be filled from routes_py.txt
import logging
import time
from typing import Optional
from fastapi import APIRouter, HTTPException, Query
from datetime import datetime

from app.services.github_service import GitHubService
from app.services.llm_service import LLMService
from app.models.schemas import (
    AnalysisRequest,
    AnalysisResponse,
    IssueAnalysis,
    PriorityScore,
    ErrorResponse,
    HealthCheckResponse,
    StatsResponse
)
from app.core.config import settings
from app.utils.cache import CacheManager
from app.utils.logger import setup_logger

logger = setup_logger(__name__)

# Initialize services
github_service = GitHubService(
    github_token=settings.GITHUB_TOKEN,
    timeout=settings.GITHUB_API_TIMEOUT
)

llm_service = LLMService(
    api_key=settings.GEMINI_API_KEY,
    model=settings.LLM_MODEL,
    temperature=settings.LLM_TEMPERATURE,
    max_tokens=settings.LLM_MAX_TOKENS
)

cache_manager = CacheManager(
    enabled=settings.REDIS_ENABLED,
    redis_host=settings.REDIS_HOST,
    redis_port=settings.REDIS_PORT,
    ttl=settings.CACHE_TTL
)

# Global statistics
stats = {
    "total_analyses": 0,
    "cache_hits": 0,
    "cache_misses": 0,
    "errors": 0,
    "response_times": []
}

# Create router
router = APIRouter(prefix="/api/v1", tags=["analysis"])


@router.post("/analyze", response_model=AnalysisResponse)
async def analyze_issue(request: AnalysisRequest) -> AnalysisResponse:
    """
    Analyze a GitHub issue and return structured insights.

    Args:
        request: AnalysisRequest with GitHub URL and issue number

    Returns:
        AnalysisResponse with analysis results

    Raises:
        HTTPException: For invalid requests or service errors
    """
    start_time = time.time()

    try:
        # Parse GitHub URL
        try:
            owner, repo = github_service.parse_github_url(request.github_url)
            logger.info(f"Parsed GitHub URL: {owner}/{repo}#{request.issue_number}")
        except ValueError as e:
            stats["errors"] += 1
            raise HTTPException(status_code=400, detail=str(e))

        # Check cache
        cache_key = f"{owner}/{repo}#{request.issue_number}"
        if request.use_cache:
            cached_result = cache_manager.get(cache_key)
            if cached_result:
                stats["cache_hits"] += 1
                logger.info(f"Cache hit for {cache_key}")
                elapsed = time.time() - start_time
                stats["response_times"].append(elapsed)

                return AnalysisResponse(
                    success=True,
                    data=IssueAnalysis(**cached_result),
                    metadata={
                        "analysis_time_ms": int(elapsed * 1000),
                        "cached": True,
                        "issue_url": f"https://github.com/{owner}/{repo}/issues/{request.issue_number}"
                    }
                )

            stats["cache_misses"] += 1

        # Validate repository exists
        is_valid = await github_service.validate_repository(owner, repo)
        if not is_valid:
            stats["errors"] += 1
            raise HTTPException(
                status_code=404,
                detail=f"Repository {owner}/{repo} not found or is private"
            )

        # Fetch issue data
        logger.info(f"Fetching issue #{request.issue_number} from {owner}/{repo}")
        github_issue = await github_service.fetch_issue(owner, repo, request.issue_number)

        # Analyze with LLM
        logger.info("Analyzing issue with Gemini API")
        analysis = await llm_service.analyze_issue(
            issue_title=github_issue.title,
            issue_body=github_issue.body,
            comments=github_issue.comments,
            repo_context=f"{owner}/{repo}"
        )

        # Cache result
        cache_manager.set(cache_key, analysis.dict())
        logger.info(f"Cached analysis result for {cache_key}")

        # Update statistics
        elapsed = time.time() - start_time
        stats["total_analyses"] += 1
        stats["response_times"].append(elapsed)

        logger.info(f"Analysis completed in {elapsed:.2f}s")

        return AnalysisResponse(
            success=True,
            data=analysis,
            metadata={
                "analysis_time_ms": int(elapsed * 1000),
                "cached": False,
                "issue_url": github_issue.url,
                "author": github_issue.author,
                "state": github_issue.state,
                "existing_labels": github_issue.labels
            }
        )

    except HTTPException:
        raise
    except Exception as e:
        stats["errors"] += 1
        logger.error(f"Unexpected error during analysis: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Internal server error")


@router.get("/health", response_model=HealthCheckResponse)
async def health_check():
    """Health check endpoint."""
    return HealthCheckResponse(
        status="healthy",
        version=settings.API_VERSION,
        timestamp=datetime.utcnow()
    )


@router.get("/stats", response_model=StatsResponse)
async def get_stats():
    """Get API statistics and analytics."""
    total = stats["total_analyses"]
    hits = stats["cache_hits"]
    misses = stats["cache_misses"]
    total_requests = hits + misses

    hit_rate = (hits / total_requests * 100) if total_requests > 0 else 0
    avg_time = (sum(stats["response_times"]) / len(stats["response_times"]) * 1000) if stats["response_times"] else 0

    return StatsResponse(
        total_analyses=total,
        cache_hits=hits,
        cache_misses=misses,
        cache_hit_rate=hit_rate,
        avg_response_time_ms=avg_time,
        errors=stats["errors"]
    )


@router.post("/analyze-batch")
async def analyze_batch(urls_and_issues: list[dict]):
    """
    Analyze multiple issues in batch (up to 5).

    Args:
        urls_and_issues: List of {github_url, issue_number} dicts

    Returns:
        List of analysis results
    """
    if len(urls_and_issues) > 5:
        raise HTTPException(status_code=400, detail="Maximum 5 issues per batch")

    results = []
    for item in urls_and_issues:
        try:
            request = AnalysisRequest(
                github_url=item["github_url"],
                issue_number=item["issue_number"]
            )
            result = await analyze_issue(request)
            results.append(result)
        except HTTPException as e:
            results.append({
                "success": False,
                "error": e.detail
            })

    return {"analyses": results, "total": len(urls_and_issues)}


@router.delete("/cache/{owner}/{repo}/{issue_number}")
async def clear_cache(owner: str, repo: str, issue_number: int):
    """Clear cache for a specific issue."""
    cache_key = f"{owner}/{repo}#{issue_number}"
    cache_manager.delete(cache_key)
    return {"success": True, "message": f"Cache cleared for {cache_key}"}


@router.post("/cache/clear-all")
async def clear_all_cache():
    """Clear entire cache (admin endpoint)."""
    cache_manager.clear_all()
    return {"success": True, "message": "All cache cleared"}


================================================
FILE: backend/app/core/__init__.py
================================================
"""
Core configuration and logging module
"""

__all__ = ["settings", "setup_logger", "get_logger"]



================================================
FILE: backend/app/core/config.py
================================================
# Configuration - to be filled from config_py.txt

import os
from typing import Optional
from functools import lru_cache
from dotenv import load_dotenv

load_dotenv()


class Settings:
    """Application configuration settings."""

    # API Keys & Tokens
    GEMINI_API_KEY: str = os.getenv("GEMINI_API_KEY", "")
    GITHUB_TOKEN: Optional[str] = os.getenv("GITHUB_TOKEN", None)

    # Server Configuration
    API_TITLE: str = "GitHub Issue Analyzer API"
    API_VERSION: str = "1.0.0"
    API_DESCRIPTION: str = "AI-powered GitHub issue analysis using Google Gemini"
    HOST: str = os.getenv("HOST", "0.0.0.0")
    PORT: int = int(os.getenv("PORT", "8000"))
    RELOAD: bool = os.getenv("RELOAD", "true").lower() == "true"
    DEBUG: bool = os.getenv("DEBUG", "false").lower() == "true"

    # CORS Configuration
    ALLOWED_ORIGINS: list = [
        "http://localhost:8501",
        "http://localhost:3000",
        "http://0.0.0.0:8501",
        os.getenv("FRONTEND_URL", "http://localhost:8501"),
    ]

    # Cache Configuration
    REDIS_ENABLED: bool = os.getenv("REDIS_ENABLED", "false").lower() == "true"
    REDIS_HOST: str = os.getenv("REDIS_HOST", "localhost")
    REDIS_PORT: int = int(os.getenv("REDIS_PORT", "6379"))
    REDIS_DB: int = int(os.getenv("REDIS_DB", "0"))
    CACHE_TTL: int = int(os.getenv("CACHE_TTL", "86400"))  # 24 hours

    # GitHub API Configuration
    GITHUB_API_BASE_URL: str = "https://api.github.com"
    GITHUB_API_TIMEOUT: int = 10
    GITHUB_RATE_LIMIT_THRESHOLD: int = 100  # Warn if below this

    # LLM Configuration
    LLM_MODEL: str = os.getenv("LLM_MODEL", "gemini-pro")
    LLM_TEMPERATURE: float = float(os.getenv("LLM_TEMPERATURE", "0.7"))
    LLM_MAX_TOKENS: int = int(os.getenv("LLM_MAX_TOKENS", "1024"))
    LLM_TIMEOUT: int = 30

    # Content Limits
    MAX_ISSUE_BODY_LENGTH: int = 8000  # Characters
    MAX_COMMENTS_COUNT: int = 20
    MAX_COMMENT_LENGTH: int = 1000  # Characters per comment

    # Request Configuration
    REQUEST_TIMEOUT: int = 30
    MAX_RETRIES: int = 3
    RETRY_DELAY: int = 1

    # Database Configuration (SQLite for simplicity)
    DATABASE_URL: str = os.getenv(
        "DATABASE_URL", "sqlite:///./github_analyzer.db"
    )

    # Logging
    LOG_LEVEL: str = os.getenv("LOG_LEVEL", "INFO")
    LOG_FORMAT: str = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

    # Feature Flags
    ENABLE_ANALYTICS: bool = True
    ENABLE_BATCH_PROCESSING: bool = True
    ENABLE_EXPORT: bool = True

    def validate(self) -> bool:
        """Validate required configuration."""
        if not self.GEMINI_API_KEY:
            raise ValueError("GEMINI_API_KEY environment variable is required")
        return True


@lru_cache()
def get_settings() -> Settings:
    """Get application settings (cached singleton)."""
    return Settings()


# Export settings instance
settings = get_settings()


================================================
FILE: backend/app/core/prompts.py
================================================
from typing import Optional

SYSTEM_PROMPT = """You are an expert GitHub issue analyzer. Your task is to analyze GitHub issues and provide structured, actionable insights.

You must ALWAYS respond with valid JSON matching this exact structure:
{
  "summary": "A one-sentence summary of the issue",
  "type": "bug|feature_request|documentation|question|other",
  "priority_score": {
    "score": 1-5,
    "justification": "Why this priority level (1-2 sentences)"
  },
  "suggested_labels": ["label1", "label2", "label3"],
  "potential_impact": "Brief sentence on impact if this is a bug"
}

CRITICAL RULES:
1. Response MUST be valid JSON only - no markdown, no code blocks
2. type MUST be one of: bug, feature_request, documentation, question, other
3. priority_score.score MUST be integer 1-5
4. suggested_labels MUST be array of 2-3 strings
5. All string values must be concise and clear"""

FEW_SHOT_EXAMPLES = [
    {
        "issue": {
            "title": "Bug: React rendering performance drops with 1000+ items",
            "body": "When rendering lists with 1000 or more items, React takes 5+ seconds to mount. This causes browser hang.",
            "comments": [
                "Same issue here with 500+ items",
                "Can reproduce on React 18.2.0",
                "Might be reconciliation algorithm issue"
            ]
        },
        "analysis": {
            "summary": "React rendering performance degrades significantly with large lists (1000+ items).",
            "type": "bug",
            "priority_score": {
                "score": 4,
                "justification": "Critical for apps handling large datasets. Blocks production usage at scale."
            },
            "suggested_labels": ["performance", "react-core", "urgent"],
            "potential_impact": "Users with large datasets experience browser hangs, poor UX."
        }
    },
    {
        "issue": {
            "title": "Feature: Add built-in dark mode support",
            "body": "Request for React to include dark mode utilities in core library. Would help with theme switching.",
            "comments": [
                "Great idea! Many apps need this",
                "Could be a separate package instead"
            ]
        },
        "analysis": {
            "summary": "Request to add native dark mode support utilities to React core.",
            "type": "feature_request",
            "priority_score": {
                "score": 2,
                "justification": "Nice-to-have feature. Can be implemented via separate packages."
            },
            "suggested_labels": ["enhancement", "style", "low-priority"],
            "potential_impact": "Would reduce boilerplate for theme management in React apps."
        }
    },
    {
        "issue": {
            "title": "Docs: useEffect cleanup function documentation unclear",
            "body": "The documentation for cleanup functions in useEffect doesn't explain when they're called. Need clarification.",
            "comments": [
                "Agree, took me days to understand this",
                "Examples would help significantly"
            ]
        },
        "analysis": {
            "summary": "Documentation for useEffect cleanup function behavior needs clarification with examples.",
            "type": "documentation",
            "priority_score": {
                "score": 3,
                "justification": "Impacts developer experience. High confusion among learners."
            },
            "suggested_labels": ["documentation", "help-wanted", "good-first-issue"],
            "potential_impact": "Clearer docs will reduce developer confusion and support questions."
        }
    }
]


def create_analysis_prompt(
    issue_title: str,
    issue_body: str,
    comments: list[str],
    repo_context: Optional[str] = None
) -> str:
    """
    Create a detailed prompt for LLM analysis with few-shot examples.

    Args:
        issue_title: Title of the GitHub issue
        issue_body: Body/description of the issue
        comments: List of comments on the issue
        repo_context: Optional repository context (e.g., "React is a UI library")

    Returns:
        Complete prompt string with few-shot examples
    """

    # Truncate long content to avoid token limits
    truncated_body = issue_body[:8000] if len(issue_body) > 8000 else issue_body
    truncated_comments = [
        c[:1000] if len(c) > 1000 else c
        for c in comments[:20]
    ]

    # Build few-shot prompt section
    few_shot_section = "\nREFERENCE EXAMPLES (follow this format exactly):\n"
    for i, example in enumerate(FEW_SHOT_EXAMPLES, 1):
        few_shot_section += f"\nEXAMPLE {i}:\n"
        few_shot_section += f"Issue Title: {example['issue']['title']}\n"
        few_shot_section += f"Issue Body: {example['issue']['body']}\n"
        few_shot_section += f"Comments: {example['issue']['comments']}\n"
        few_shot_section += f"Analysis Result:\n{example['analysis']}\n"

    # Build the main analysis prompt
    prompt = f"""{SYSTEM_PROMPT}

{few_shot_section}

---

NOW ANALYZE THIS ISSUE:

Repository: {repo_context or 'Unknown'}

Issue Title: {issue_title}

Issue Body:
{truncated_body}

Comments from Community:
{chr(10).join(f'- {c}' for c in truncated_comments) if truncated_comments else '(No comments yet)'}

---

Provide ONLY the JSON analysis result, no additional text:
"""

    return prompt


def create_batch_analysis_prompt(issues: list[dict]) -> str:
    """
    Create a prompt for batch analysis of multiple issues.

    Args:
        issues: List of issue dictionaries

    Returns:
        Complete batch analysis prompt
    """

    issues_section = "\n".join([
        f"""
Issue #{i+1}:
- Title: {issue['title']}
- Body: {issue['body'][:500]}...
- Comments: {len(issue.get('comments', []))} comments
"""
        for i, issue in enumerate(issues[:5])  # Max 5 for batch
    ])

    prompt = f"""{SYSTEM_PROMPT}

Analyze the following {len(issues)} issues and return a JSON array with analysis for each:

{issues_section}

Return format: [{{"summary": "...", "type": "...", ...}}, {{"summary": "...", "type": "...", ...}}]
"""

    return prompt


def create_summary_prompt(analyses: list[dict]) -> str:
    """
    Create a prompt to summarize multiple analyses.

    Args:
        analyses: List of analysis results

    Returns:
        Summary prompt
    """

    prompt = f"""Based on these {len(analyses)} GitHub issue analyses, provide:
1. Top 3 most critical issues
2. Common issue patterns
3. Recommended prioritization strategy

Analyses:
{analyses}

Provide response as a JSON object with keys: critical_issues, patterns, prioritization.
"""

    return prompt


================================================
FILE: backend/app/models/__init__.py
================================================
"""
Models module - Pydantic schemas and data models
"""

__all__ = ["AnalysisRequest", "AnalysisResponse", "IssueAnalysis"]



================================================
FILE: backend/app/models/schemas.py
================================================
# Data schemas - to be filled from schemas_py.txt
from pydantic import BaseModel, Field, HttpUrl, validator
from typing import List, Optional, Dict, Any
from datetime import datetime


class AnalysisRequest(BaseModel):
    """Request model for issue analysis."""
    github_url: str = Field(
        ...,
        description="Public GitHub repository URL (e.g., https://github.com/facebook/react)"
    )
    issue_number: int = Field(
        ...,
        gt=0,
        description="GitHub issue number"
    )
    use_cache: bool = Field(
        default=True,
        description="Use cached results if available"
    )

    @validator("github_url")
    def validate_github_url(cls, v):
        """Validate GitHub URL format."""
        if "github.com" not in v:
            raise ValueError("Must be a valid GitHub URL")
        return v


class PriorityScore(BaseModel):
    """Priority score with justification."""
    score: int = Field(
        ...,
        ge=1,
        le=5,
        description="Priority score from 1 (low) to 5 (critical)"
    )
    justification: str = Field(
        ...,
        description="Brief justification for the score"
    )


class IssueAnalysis(BaseModel):
    """Complete issue analysis result."""
    summary: str = Field(
        ...,
        description="One-sentence summary of the issue"
    )
    type: str = Field(
        ...,
        description="Issue type: bug, feature_request, documentation, question, other"
    )
    priority_score: PriorityScore = Field(
        ...,
        description="Priority score with justification"
    )
    suggested_labels: List[str] = Field(
        default_factory=list,
        description="Suggested GitHub labels (2-3 items)"
    )
    potential_impact: str = Field(
        ...,
        description="Potential impact of the issue"
    )

    @validator("type")
    def validate_type(cls, v):
        """Validate issue type."""
        valid_types = ["bug", "feature_request", "documentation", "question", "other"]
        if v not in valid_types:
            raise ValueError(f"Type must be one of: {', '.join(valid_types)}")
        return v


class AnalysisResponse(BaseModel):
    """Complete API response."""
    success: bool
    data: Optional[IssueAnalysis] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)
    error: Optional[str] = None
    message: Optional[str] = None

    class Config:
        json_schema_extra = {
            "example": {
                "success": True,
                "data": {
                    "summary": "Users want faster rendering performance",
                    "type": "bug",
                    "priority_score": {
                        "score": 4,
                        "justification": "Impacts user experience significantly"
                    },
                    "suggested_labels": ["performance", "urgent"],
                    "potential_impact": "Slow rendering affects app usability"
                },
                "metadata": {
                    "analysis_time_ms": 2150,
                    "cached": False,
                    "issue_url": "https://github.com/facebook/react/issues/28029"
                }
            }
        }


class HealthCheckResponse(BaseModel):
    """Health check response."""
    status: str = Field(..., description="Service status")
    version: str = Field(..., description="API version")
    timestamp: datetime = Field(..., description="Check timestamp")


class StatsResponse(BaseModel):
    """Analytics statistics."""
    total_analyses: int
    cache_hits: int
    cache_misses: int
    cache_hit_rate: float
    avg_response_time_ms: float
    errors: int


class ErrorResponse(BaseModel):
    """Error response model."""
    success: bool = False
    error: str
    details: Optional[str] = None
    timestamp: datetime = Field(default_factory=datetime.utcnow)


================================================
FILE: backend/app/services/__init__.py
================================================
"""
Services module - GitHub, LLM, and Cache services
"""

__all__ = ["GitHubService", "LLMService", "CacheService"]



================================================
FILE: backend/app/services/github_service.py
================================================
import httpx
import logging
from typing import Optional, Dict, Any, List
from dataclasses import dataclass
from urllib.parse import urlparse
import asyncio

logger = logging.getLogger(__name__)


@dataclass
class GitHubIssue:
    """Structured GitHub issue data."""
    title: str
    body: str
    comments: List[str]
    url: str
    state: str
    labels: List[str]
    created_at: str
    updated_at: str
    author: str
    reactions: Dict[str, int]


class GitHubService:
    """Service for interacting with GitHub API."""

    def __init__(
        self,
        github_token: Optional[str] = None,
        timeout: int = 10,
        max_retries: int = 3
    ):
        """
        Initialize GitHub service.

        Args:
            github_token: Optional GitHub personal access token
            timeout: Request timeout in seconds
            max_retries: Maximum retry attempts for failed requests
        """
        self.github_token = github_token
        self.timeout = timeout
        self.max_retries = max_retries
        self.base_url = "https://api.github.com"
        self.headers = self._build_headers()

    def _build_headers(self) -> Dict[str, str]:
        """Build request headers with authentication."""
        headers = {
            "Accept": "application/vnd.github.v3+json",
            "User-Agent": "GitHub-Issue-Analyzer/1.0"
        }
        if self.github_token:
            headers["Authorization"] = f"token {self.github_token}"
        return headers

    def parse_github_url(self, url: str) -> tuple[str, str]:
        """
        Parse GitHub URL to extract owner and repo.

        Args:
            url: GitHub repository URL

        Returns:
            Tuple of (owner, repo)

        Raises:
            ValueError: If URL format is invalid
        """
        try:
            # Handle various URL formats
            url = url.strip().rstrip('/')

            # Extract from https://github.com/owner/repo
            if "github.com" in url:
                parts = url.split('/')
                if len(parts) >= 2:
                    owner = parts[-2]
                    repo = parts[-1].replace('.git', '')
                    if owner and repo:
                        return owner, repo

            raise ValueError(f"Invalid GitHub URL format: {url}")
        except Exception as e:
            logger.error(f"Error parsing GitHub URL: {e}")
            raise ValueError(f"Invalid GitHub URL: {url}") from e

    async def fetch_issue(
        self,
        owner: str,
        repo: str,
        issue_number: int
    ) -> GitHubIssue:
        """
        Fetch GitHub issue with comments.

        Args:
            owner: Repository owner
            repo: Repository name
            issue_number: Issue number

        Returns:
            GitHubIssue object with all data

        Raises:
            Exception: If API request fails
        """
        async with httpx.AsyncClient(timeout=self.timeout) as client:
            try:
                # Fetch issue
                issue_url = (
                    f"{self.base_url}/repos/{owner}/{repo}/issues/{issue_number}"
                )
                issue_response = await client.get(
                    issue_url,
                    headers=self.headers
                )
                issue_response.raise_for_status()
                issue_data = issue_response.json()

                # Fetch comments
                comments = await self._fetch_comments(
                    client, owner, repo, issue_number
                )

                # Check rate limit
                rate_limit = issue_response.headers.get("X-RateLimit-Remaining")
                if rate_limit:
                    logger.info(f"GitHub API rate limit remaining: {rate_limit}")

                return GitHubIssue(
                    title=issue_data.get("title", ""),
                    body=issue_data.get("body", ""),
                    comments=comments,
                    url=issue_data.get("html_url", ""),
                    state=issue_data.get("state", ""),
                    labels=[label["name"] for label in issue_data.get("labels", [])],
                    created_at=issue_data.get("created_at", ""),
                    updated_at=issue_data.get("updated_at", ""),
                    author=issue_data.get("user", {}).get("login", ""),
                    reactions=issue_data.get("reactions", {})
                )

            except httpx.HTTPStatusError as e:
                if e.response.status_code == 404:
                    raise ValueError(f"Issue #{issue_number} not found in {owner}/{repo}")
                elif e.response.status_code == 403:
                    raise ValueError("GitHub API rate limit exceeded. Please try again later.")
                else:
                    logger.error(f"GitHub API error: {e}")
                    raise

            except Exception as e:
                logger.error(f"Error fetching GitHub issue: {e}")
                raise

    async def _fetch_comments(
        self,
        client: httpx.AsyncClient,
        owner: str,
        repo: str,
        issue_number: int,
        max_comments: int = 20
    ) -> List[str]:
        """
        Fetch issue comments from GitHub API.

        Args:
            client: HTTPX async client
            owner: Repository owner
            repo: Repository name
            issue_number: Issue number
            max_comments: Maximum comments to fetch

        Returns:
            List of comment texts
        """
        try:
            comments_url = (
                f"{self.base_url}/repos/{owner}/{repo}/issues/{issue_number}/comments"
            )
            comments_response = await client.get(
                comments_url,
                headers=self.headers,
                params={"per_page": max_comments}
            )
            comments_response.raise_for_status()
            comments_data = comments_response.json()

            # Extract comment bodies
            comments = [
                comment.get("body", "")
                for comment in comments_data
                if comment.get("body")
            ]

            logger.info(f"Fetched {len(comments)} comments")
            return comments[:max_comments]

        except Exception as e:
            logger.warning(f"Error fetching comments: {e}")
            return []

    async def get_rate_limit_status(self) -> Dict[str, Any]:
        """
        Get current GitHub API rate limit status.

        Returns:
            Rate limit information
        """
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.get(
                    f"{self.base_url}/rate_limit",
                    headers=self.headers
                )
                response.raise_for_status()
                return response.json()
        except Exception as e:
            logger.error(f"Error fetching rate limit: {e}")
            return {}

    async def validate_repository(
        self,
        owner: str,
        repo: str
    ) -> bool:
        """
        Validate that a repository exists and is accessible.

        Args:
            owner: Repository owner
            repo: Repository name

        Returns:
            True if repository exists and is accessible
        """
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.get(
                    f"{self.base_url}/repos/{owner}/{repo}",
                    headers=self.headers
                )
                return response.status_code == 200
        except Exception:
            return False



================================================
FILE: backend/app/services/llm_service.py
================================================
# LLM service - to be filled from llm_service_py.txt
import json
import logging
import re
from typing import Optional, Dict, Any
import google.generativeai as genai
from google.api_core import retry

from app.core.prompts import create_analysis_prompt
from app.models.schemas import IssueAnalysis, PriorityScore

logger = logging.getLogger(__name__)


class LLMService:
    """Service for LLM-based issue analysis using Google Gemini."""

    def __init__(
        self,
        api_key: str,
        model: str = "gemini-pro",
        temperature: float = 0.7,
        max_tokens: int = 1024,
        timeout: int = 30
    ):
        """
        Initialize LLM service.

        Args:
            api_key: Google Gemini API key
            model: Model name (e.g., 'gemini-pro')
            temperature: Temperature for generation (0-1)
            max_tokens: Maximum tokens in response
            timeout: Request timeout in seconds
        """
        self.api_key = api_key
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.timeout = timeout

        # Configure Gemini API
        genai.configure(api_key=api_key)
        self.client = genai.GenerativeModel(model)

    async def analyze_issue(
        self,
        issue_title: str,
        issue_body: str,
        comments: list[str],
        repo_context: Optional[str] = None,
        retry_count: int = 0,
        max_retries: int = 3
    ) -> IssueAnalysis:
        """
        Analyze a GitHub issue using LLM.

        Args:
            issue_title: Title of the issue
            issue_body: Body/description of the issue
            comments: List of comments
            repo_context: Optional repository context
            retry_count: Current retry attempt
            max_retries: Maximum retry attempts

        Returns:
            IssueAnalysis object with structured results

        Raises:
            ValueError: If analysis fails after retries
        """
        try:
            # Create prompt with few-shot examples
            prompt = create_analysis_prompt(
                issue_title=issue_title,
                issue_body=issue_body,
                comments=comments,
                repo_context=repo_context
            )

            # Call Gemini API
            response = await self._call_gemini(prompt)
            logger.info("Received response from Gemini API")

            # Parse and validate response
            analysis = self._parse_response(response)

            logger.info(f"Successfully analyzed issue: {analysis.summary}")
            return analysis

        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing error: {e}. Response: {response}")
            if retry_count < max_retries:
                logger.info(f"Retrying analysis (attempt {retry_count + 1}/{max_retries})")
                # Add clearer instructions to prompt
                enhanced_prompt = prompt + "\n\nIMPORTANT: Respond with ONLY valid JSON, no other text."
                response = await self._call_gemini(enhanced_prompt)
                return await self.analyze_issue(
                    issue_title=issue_title,
                    issue_body=issue_body,
                    comments=comments,
                    repo_context=repo_context,
                    retry_count=retry_count + 1,
                    max_retries=max_retries
                )
            else:
                raise ValueError(f"Failed to parse LLM response after {max_retries} retries")

        except Exception as e:
            logger.error(f"Error analyzing issue: {e}")
            if retry_count < max_retries:
                logger.info(f"Retrying after error (attempt {retry_count + 1}/{max_retries})")
                return await self.analyze_issue(
                    issue_title=issue_title,
                    issue_body=issue_body,
                    comments=comments,
                    repo_context=repo_context,
                    retry_count=retry_count + 1,
                    max_retries=max_retries
                )
            raise

    async def _call_gemini(self, prompt: str) -> str:
        """
        Call Google Gemini API.

        Args:
            prompt: The analysis prompt

        Returns:
            API response text
        """
        try:
            # Use synchronous client since Gemini doesn't have proper async support yet
            response = self.client.generate_content(
                prompt,
                generation_config={
                    "temperature": self.temperature,
                    "max_output_tokens": self.max_tokens,
                }
            )

            if response.text:
                return response.text
            else:
                raise ValueError("Empty response from Gemini API")

        except Exception as e:
            logger.error(f"Gemini API error: {e}")
            raise

    def _parse_response(self, response_text: str) -> IssueAnalysis:
        """
        Parse and validate LLM response.

        Args:
            response_text: Raw response from LLM

        Returns:
            Validated IssueAnalysis object

        Raises:
            ValueError: If response format is invalid
        """
        try:
            # Extract JSON from response (handle markdown code blocks)
            json_match = re.search(
                r'```(?:json)?\s*(.*?)\s*```',
                response_text,
                re.DOTALL
            )

            if json_match:
                json_str = json_match.group(1)
            else:
                json_str = response_text

            # Parse JSON
            data = json.loads(json_str)

            # Validate and construct IssueAnalysis
            analysis = IssueAnalysis(
                summary=data.get("summary", ""),
                type=self._validate_type(data.get("type")),
                priority_score=PriorityScore(
                    score=self._validate_priority_score(data.get("priority_score", {}).get("score")),
                    justification=data.get("priority_score", {}).get("justification", "")
                ),
                suggested_labels=self._validate_labels(data.get("suggested_labels", [])),
                potential_impact=data.get("potential_impact", "")
            )

            return analysis

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON: {e}")
            raise ValueError(f"Invalid JSON in response: {e}")
        except KeyError as e:
            logger.error(f"Missing required field: {e}")
            raise ValueError(f"Missing required field in response: {e}")

    @staticmethod
    def _validate_type(issue_type: str) -> str:
        """Validate issue type."""
        valid_types = ["bug", "feature_request", "documentation", "question", "other"]
        if issue_type not in valid_types:
            logger.warning(f"Invalid type '{issue_type}', defaulting to 'other'")
            return "other"
        return issue_type

    @staticmethod
    def _validate_priority_score(score: Any) -> int:
        """Validate and sanitize priority score."""
        try:
            score_int = int(score)
            if 1 <= score_int <= 5:
                return score_int
            else:
                logger.warning(f"Priority score {score} out of range, defaulting to 3")
                return 3
        except (ValueError, TypeError):
            logger.warning(f"Invalid priority score {score}, defaulting to 3")
            return 3

    @staticmethod
    def _validate_labels(labels: Any) -> list[str]:
        """Validate and sanitize labels."""
        if not isinstance(labels, list):
            return []

        # Keep only strings, limit to 5 labels
        valid_labels = [
            str(label).strip()
            for label in labels
            if isinstance(label, (str, int))
        ][:5]

        return valid_labels if valid_labels else ["general"]

    async def analyze_batch(
        self,
        issues: list[Dict[str, Any]]
    ) -> list[IssueAnalysis]:
        """
        Analyze multiple issues in batch.

        Args:
            issues: List of issue dictionaries

        Returns:
            List of IssueAnalysis objects
        """
        results = []
        for issue in issues[:5]:  # Max 5 for batch
            try:
                analysis = await self.analyze_issue(
                    issue_title=issue.get("title", ""),
                    issue_body=issue.get("body", ""),
                    comments=issue.get("comments", []),
                    repo_context=issue.get("repo", None)
                )
                results.append(analysis)
            except Exception as e:
                logger.error(f"Error analyzing issue in batch: {e}")
                continue

        return results


================================================
FILE: backend/app/utils/__init__.py
================================================
"""
Utils module - Utility functions, cache, validators, and helpers
"""

__all__ = ["validate_github_url", "validate_issue_number", "Cache", "CacheManager"]



================================================
FILE: backend/app/utils/cache.py
================================================
# Caching utilities - to be filled from cache_py.txt
import json
import logging
from typing import Optional, Dict, Any
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)


class CacheManager:
    """Cache manager with Redis support and in-memory fallback."""

    def __init__(
        self,
        enabled: bool = False,
        redis_host: str = "localhost",
        redis_port: int = 6379,
        redis_db: int = 0,
        ttl: int = 86400
    ):
        """
        Initialize cache manager.

        Args:
            enabled: Enable Redis caching
            redis_host: Redis server host
            redis_port: Redis server port
            redis_db: Redis database number
            ttl: Cache TTL in seconds
        """
        self.enabled = enabled
        self.ttl = ttl
        self.redis_client = None
        self.in_memory_cache: Dict[str, Dict[str, Any]] = {}

        if enabled:
            try:
                import redis
                self.redis_client = redis.Redis(
                    host=redis_host,
                    port=redis_port,
                    db=redis_db,
                    decode_responses=True
                )
                self.redis_client.ping()
                logger.info("Redis cache initialized successfully")
            except Exception as e:
                logger.warning(f"Failed to initialize Redis: {e}. Using in-memory cache.")
                self.enabled = False

    def get(self, key: str) -> Optional[Dict[str, Any]]:
        """
        Get value from cache.

        Args:
            key: Cache key

        Returns:
            Cached value or None
        """
        if self.redis_client:
            try:
                value = self.redis_client.get(key)
                if value:
                    logger.debug(f"Cache hit: {key}")
                    return json.loads(value)
            except Exception as e:
                logger.warning(f"Redis get error: {e}")

        # In-memory fallback
        if key in self.in_memory_cache:
            entry = self.in_memory_cache[key]
            if entry["expires_at"] > datetime.utcnow():
                logger.debug(f"In-memory cache hit: {key}")
                return entry["value"]
            else:
                del self.in_memory_cache[key]

        return None

    def set(
        self,
        key: str,
        value: Dict[str, Any],
        ttl: Optional[int] = None
    ) -> bool:
        """
        Set value in cache.

        Args:
            key: Cache key
            value: Value to cache
            ttl: Optional TTL override

        Returns:
            True if successful
        """
        ttl = ttl or self.ttl

        if self.redis_client:
            try:
                self.redis_client.setex(
                    key,
                    ttl,
                    json.dumps(value)
                )
                logger.debug(f"Redis cache set: {key}")
                return True
            except Exception as e:
                logger.warning(f"Redis set error: {e}")

        # In-memory fallback
        self.in_memory_cache[key] = {
            "value": value,
            "expires_at": datetime.utcnow() + timedelta(seconds=ttl)
        }
        logger.debug(f"In-memory cache set: {key}")
        return True

    def delete(self, key: str) -> bool:
        """Delete cache entry."""
        if self.redis_client:
            try:
                self.redis_client.delete(key)
                return True
            except Exception as e:
                logger.warning(f"Redis delete error: {e}")

        if key in self.in_memory_cache:
            del self.in_memory_cache[key]
            return True

        return False

    def clear_all(self) -> bool:
        """Clear entire cache."""
        if self.redis_client:
            try:
                self.redis_client.flushdb()
                logger.info("Redis cache cleared")
                return True
            except Exception as e:
                logger.warning(f"Redis clear error: {e}")

        self.in_memory_cache.clear()
        logger.info("In-memory cache cleared")
        return True

    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics."""
        if self.redis_client:
            try:
                info = self.redis_client.info()
                return {
                    "type": "redis",
                    "keys": info.get("db0", {}).get("keys", 0),
                    "memory_usage": info.get("used_memory_human", "N/A")
                }
            except Exception as e:
                logger.warning(f"Error getting Redis stats: {e}")

        return {
            "type": "in-memory",
            "keys": len(self.in_memory_cache),
            "memory_usage": "N/A"
        }


================================================
FILE: backend/app/utils/logger.py
================================================
# Logging utilities - to be filled from logger_py.txt
import logging
from rich.logging import RichHandler
from rich.console import Console
from app.core.config import settings

# Create console for Rich output
console = Console()


def setup_logger(name: str) -> logging.Logger:
    """
    Setup logger with Rich handler for pretty output.

    Args:
        name: Logger name

    Returns:
        Configured logger instance
    """
    logger = logging.getLogger(name)

    if not logger.handlers:
        handler = RichHandler(
            rich_tracebacks=True,
            markup=True,
            show_time=True,
            show_level=True,
            show_path=False,
            console=console
        )

        formatter = logging.Formatter(
            fmt="%(message)s",
            datefmt="%Y-%m-%d %H:%M:%S"
        )

        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.setLevel(settings.LOG_LEVEL)

    return logger


================================================
FILE: frontend/app.py
================================================
# Streamlit app - to be filled from streamlit_app_py.txt
import streamlit as st
import httpx
import json
from datetime import datetime
from typing import Optional, Dict, Any
import time

# Page configuration
st.set_page_config(
    page_title="GitHub Issue Analyzer",
    page_icon="🔍",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Apply custom CSS
st.markdown("""
<style>
    .main {
        padding: 2rem;
    }
    .stButton > button {
        width: 100%;
        height: 2.5rem;
        font-size: 1rem;
    }
    .result-container {
        border-left: 5px solid #1f77e2;
        padding: 1rem;
        border-radius: 0.5rem;
        background-color: #f0f4f8;
    }
    .priority-critical {
        color: #d62728;
        font-weight: bold;
    }
    .priority-high {
        color: #ff7f0e;
        font-weight: bold;
    }
    .priority-medium {
        color: #2ca02c;
        font-weight: bold;
    }
    .priority-low {
        color: #1f77e2;
        font-weight: bold;
    }
</style>
""", unsafe_allow_html=True)

# Initialize session state
if "api_url" not in st.session_state:
    st.session_state.api_url = "http://localhost:8000"
if "analysis_result" not in st.session_state:
    st.session_state.analysis_result = None
if "is_loading" not in st.session_state:
    st.session_state.is_loading = False


def get_priority_color(score: int) -> str:
    """Get CSS class for priority color."""
    if score >= 4:
        return "priority-critical"
    elif score == 3:
        return "priority-high"
    elif score == 2:
        return "priority-medium"
    else:
        return "priority-low"


def call_backend_api(
    github_url: str,
    issue_number: int,
    use_cache: bool = True
) -> Optional[Dict[str, Any]]:
    """Call the backend API for issue analysis."""
    try:
        with httpx.Client(timeout=60.0) as client:
            response = client.post(
                f"{st.session_state.api_url}/api/v1/analyze",
                json={
                    "github_url": github_url,
                    "issue_number": issue_number,
                    "use_cache": use_cache
                }
            )
            response.raise_for_status()
            return response.json()
    except httpx.ConnectError:
        st.error(
            "❌ Cannot connect to backend API. "
            "Make sure the FastAPI server is running on http://localhost:8000"
        )
        return None
    except httpx.HTTPStatusError as e:
        error_detail = e.response.json().get("detail", str(e))
        st.error(f"❌ API Error: {error_detail}")
        return None
    except Exception as e:
        st.error(f"❌ Error: {str(e)}")
        return None


def display_analysis_result(result: Dict[str, Any]) -> None:
    """Display the analysis result in a formatted way."""
    if not result.get("success"):
        st.error(f"Analysis failed: {result.get('error', 'Unknown error')}")
        return

    data = result.get("data", {})
    metadata = result.get("metadata", {})

    # Main result container
    col1, col2 = st.columns([3, 1])

    with col1:
        st.markdown("### 📊 Analysis Results")

        # Summary
        st.markdown(f"""
        **Summary:** {data.get('summary', 'N/A')}
        """)

        # Type and Priority in columns
        type_col, priority_col = st.columns(2)

        with type_col:
            issue_type = data.get("type", "unknown").upper()
            st.markdown(f"**Issue Type:** `{issue_type}`")

        with priority_col:
            priority_score = data.get("priority_score", {})
            score = priority_score.get("score", 0)
            priority_class = get_priority_color(score)
            st.markdown(f"**Priority:** <span class='{priority_class}'>{score}/5</span>", unsafe_allow_html=True)

        # Priority Justification
        st.markdown(f"""
        **Justification:** {priority_score.get('justification', 'N/A')}
        """)

        # Suggested Labels
        labels = data.get("suggested_labels", [])
        if labels:
            st.markdown("**Suggested Labels:**")
            label_cols = st.columns(len(labels))
            for i, label in enumerate(labels):
                with label_cols[i]:
                    st.markdown(f"🏷️ `{label}`")

        # Potential Impact
        st.markdown(f"""
        **Potential Impact:** {data.get('potential_impact', 'N/A')}
        """)

    with col2:
        st.markdown("### ⏱️ Metadata")
        if metadata:
            st.metric(
                "Analysis Time",
                f"{metadata.get('analysis_time_ms', 'N/A')} ms"
            )
            cached = "✓ Yes" if metadata.get('cached') else "✗ No"
            st.metric("From Cache", cached)

            if metadata.get('existing_labels'):
                st.markdown("**Existing Labels:**")
                for label in metadata['existing_labels']:
                    st.caption(label)

    # GitHub Issue Link
    if metadata.get('issue_url'):
        st.markdown(f"""
        ---
        [🔗 View on GitHub]({metadata['issue_url']})
        """)


def main():
    """Main Streamlit application."""
    # Header
    st.title("🔍 GitHub Issue Analyzer")
    st.markdown("""
    Analyze GitHub issues with AI-powered insights powered by Google Gemini.
    Get intelligent summaries, priority scores, and suggested labels instantly.
    """)

    # Sidebar Configuration
    with st.sidebar:
        st.markdown("### ⚙️ Configuration")

        # Backend URL
        backend_url = st.text_input(
            "Backend API URL",
            value=st.session_state.api_url,
            placeholder="http://localhost:8000"
        )
        st.session_state.api_url = backend_url

        # Check backend health
        if st.button("🏥 Check Backend Health"):
            try:
                with httpx.Client(timeout=5.0) as client:
                    response = client.get(f"{backend_url}/api/v1/health")
                    if response.status_code == 200:
                        st.success("✓ Backend is healthy!")
                    else:
                        st.error(f"Backend returned status {response.status_code}")
            except Exception as e:
                st.error(f"Cannot connect to backend: {str(e)}")

        st.divider()

        # Statistics
        if st.button("📊 View API Statistics"):
            try:
                with httpx.Client(timeout=5.0) as client:
                    response = client.get(f"{backend_url}/api/v1/stats")
                    if response.status_code == 200:
                        stats = response.json()
                        st.metric("Total Analyses", stats.get("total_analyses", 0))
                        st.metric("Cache Hit Rate", f"{stats.get('cache_hit_rate', 0):.1f}%")
                        st.metric("Avg Response Time", f"{stats.get('avg_response_time_ms', 0):.0f} ms")
            except Exception as e:
                st.warning(f"Could not fetch statistics: {str(e)}")

        st.divider()

        # Documentation
        with st.expander("📚 Help & Documentation"):
            st.markdown("""
            ### How to use:
            1. Enter a GitHub repository URL (e.g., https://github.com/facebook/react)
            2. Enter the issue number you want to analyze
            3. Click "Analyze Issue" to get instant insights
            
            ### Features:
            - **Summary**: One-sentence issue summary
            - **Type Classification**: bug, feature_request, documentation, question, other
            - **Priority Scoring**: 1-5 score with justification
            - **Label Suggestions**: Recommended GitHub labels
            - **Impact Analysis**: Potential impact assessment
            
            ### Powered by:
            - **Backend**: FastAPI with async/await
            - **LLM**: Google Gemini Pro
            - **Caching**: Redis (optional) / In-Memory
            """)

    # Main content area
    st.markdown("### 🚀 Analyze an Issue")

    # Input form
    col1, col2 = st.columns([3, 1])

    with col1:
        github_url = st.text_input(
            "GitHub Repository URL",
            placeholder="https://github.com/facebook/react",
            help="Enter the full GitHub repository URL"
        )

    with col2:
        issue_number = st.number_input(
            "Issue Number",
            min_value=1,
            value=1,
            help="Enter the GitHub issue number"
        )

    # Use cache checkbox
    use_cache = st.checkbox("Use cached results if available", value=True)

    # Analyze button
    if st.button("🔍 Analyze Issue", use_container_width=True):
        if not github_url or not issue_number:
            st.error("Please fill in all fields")
        else:
            with st.spinner("🔄 Analyzing issue..."):
                result = call_backend_api(github_url, int(issue_number), use_cache)
                if result:
                    st.session_state.analysis_result = result
                    st.success("✓ Analysis complete!")

    # Display results
    if st.session_state.analysis_result:
        st.divider()
        display_analysis_result(st.session_state.analysis_result)

        # Export options
        col1, col2, col3 = st.columns(3)

        with col1:
            if st.button("📋 Copy as JSON"):
                st.code(
                    json.dumps(st.session_state.analysis_result, indent=2),
                    language="json"
                )

        with col2:
            # Download as JSON
            json_str = json.dumps(st.session_state.analysis_result, indent=2)
            st.download_button(
                label="⬇️ Download JSON",
                data=json_str,
                file_name=f"analysis_{int(time.time())}.json",
                mime="application/json"
            )

        with col3:
            if st.button("🔄 Clear Results"):
                st.session_state.analysis_result = None
                st.rerun()


if __name__ == "__main__":
    main()


================================================
FILE: frontend/pages/analytics.py
================================================
# Analytics page for multi-page Streamlit app



================================================
FILE: frontend/pages/home.py
================================================
# Home page for multi-page Streamlit app



================================================
FILE: frontend/utils/__init__.py
================================================
[Empty file]


================================================
FILE: frontend/utils/api_client.py
================================================
# API client helper functions



================================================
FILE: frontend/utils/formatters.py
================================================
# Display formatting helpers



================================================
FILE: scripts/deploy.sh
================================================
#!/bin/bash
# Deployment script for GitHub Issue Analyzer



================================================
FILE: scripts/setup.sh
================================================
#!/bin/bash
# Setup script for GitHub Issue Analyzer



================================================
FILE: tests/__init__.py
================================================
[Empty file]


================================================
FILE: tests/test_api.py
================================================
# Tests for API routes



================================================
FILE: tests/test_github_service.py
================================================
# Tests for GitHub service



================================================
FILE: tests/test_llm_service.py
================================================
# Tests for LLM service


